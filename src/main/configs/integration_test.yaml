paths:
  videos_folder: /home/waseem/data/20bn-somethingsomething-2017-10-31
  checkpoint_folder: checkpoint_sample/

  train_annot: &train_annot fake_data/json/fake.json
  validation_annot: *train_annot
  test_annot: *train_annot

pretrained:
  pretrained_folder:
  pretrained_file:

device:
  synchronize_gpu: True
  gpus:

dataloaders:
  kwargs: {batch_size: 2, num_workers: 16, pin_memory: False}

model:
  type: EncoderDecoder
  encoder: CNN3dLSTMEncoder
  encoder_args:
  encoder_kwargs: {encoder_output_size: &enc_out_size 25}
  decoder: LSTMDecoder
  decoder_args:
  decoder_kwargs: {embedding_size: 25,
                   hidden_size: *enc_out_size,
                   num_lstm_layers: 1}


loss:
  type: SequenceCrossEntropy

optimizer:
  type: Adam
  kwargs: {lr: 0.01}

scheduler:
  type: ReduceLROnPlateau
  kwargs: {factor: 0.1,
           min_lr: 1.0e-4,
           patience: 1,
           threshold: 1.0e-4,
           verbose: True}

training:
  num_epochs: 2
  clip_grad: 1
  teacher_force: True
  verbose: True

validation:
  frequency: 1
  teacher_force: False
  verbose: True

criteria:
  score: loss
  higher_is_better: False

targets:
  caption_type: template

tokenizer:
  kwargs: {user_maxlen: ~, cutoff: 0}


logging:
  verbose: True
  tensorboard_frequency: 1
