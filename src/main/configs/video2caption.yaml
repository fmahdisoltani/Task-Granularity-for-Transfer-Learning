paths:
  videos_folder: &vidpath /Users/farzaneh/PycharmProjects/TwentyBN/pytorch-captioning/sample_data/json

  annot_folder: /Users/farzaneh/PycharmProjects/TwentyBN/pytorch-captioning/sample_data/json
  train_annot: &train_annot /Users/farzaneh/PycharmProjects/TwentyBN/pytorch-captioning/sample_data/json/training_sample.json
  validation_annot: *train_annot
  test_annot: *train_annot
  checkpoint: &ckpt /model_checkpoints/vid2caption_001

training:
  num_epochs: 200
  learning_rate: &lr 0.001
  patience: &pat 10
  resume: False
  pretrained_model: /path/to/checkpoint/dir
  verbose: True

device:
  use_cuda: &use_cuda True
  synchronize_gpu: &sync True
  gpus: [0]

datasets:
  token_min_count: 1
  video_preprocessing:
    train:
      type: random_crop_preprocessing
      args: &trn_args [[36, 96, 96], 64.]
    validation: &val_vid_prep
      type: crop_center_preprocessing
      args: *trn_args
    test: *val_vid_prep
  kwargs: {}

dataloaders:
  batch_size: &b 16
  num_batches_per_epoch: 2500
  kwargs: {batch_size: *b, num_workers: 16, pin_memory: True}

model:
  encoder: FullyConnected
  mapper: FullyConnected
  decoder: FullyConnected

loss:
  type: SequenceCrossEntropy
  args: []

optimizer:
  type: Adam
  kwargs: {lr: *lr}