{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import importlib\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import HTML\n",
    "from skvideo.io import FFmpegReader, ffprobe, vwrite\n",
    "from torch.autograd import Variable\n",
    "from ptcap.trainers import DataParallelWrapper\n",
    "from ptcap.grad_cam_videos import GradCam\n",
    "from ptcap.data.annotation_parser import V2Parser\n",
    "\n",
    "sys.path.insert(0, \"../\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ptcap.model.captioners import EncoderDecoder\n",
    "from ptcap.model.encoders import C3dLSTMEncoder\n",
    "from ptcap.model.two_stream_encoders import TwoStreamEncoder\n",
    "from ptcap.model.external_encoders import FCEncoder, JesterEncoder, BIJesterEncoder\n",
    "from ptcap.model.decoders import LSTMDecoder, CoupledLSTMDecoder\n",
    "\n",
    "net = EncoderDecoder(\n",
    "        encoder=TwoStreamEncoder,\n",
    "        decoder=CoupledLSTMDecoder,\n",
    "        encoder_kwargs={\"encoder_output_size\": 1024,\"c2d_out_ch\": 32,\n",
    "                   \"c3d_out_ch\": 32, \"rnn_output_size\":1024},#, \"pretrained_path\": \"/home/farzaneh/PycharmProjects/pretrained_nets/fully_conv_net_on_smtsmt_20170627/model.checkpoint\"},\n",
    "        decoder_kwargs={\"embedding_size\": 256, \"hidden_size\": 1024, \"num_lstm_layers\": 2, \n",
    "        \"vocab_size\": 2728, \"num_step\" :17, \"fc_size\":1024}, \n",
    "        gpus=[0]).cuda()\n",
    "net = DataParallelWrapper(net, device_ids=[0]).cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = '/home/farzaneh/PycharmProjects/pytorch-captioning/results/clapnet_balanced_tokens/'\n",
    "path = '/home/farzaneh/PycharmProjects/pytorch-captioning/results/ECCV/v2_gulp160_two_stream_c2_32_c3_32_labels_cutoff5_cassif1_cap0/'\n",
    "\n",
    "checkpoint = torch.load(path + '/model.best')\n",
    "\n",
    "\n",
    "net.load_state_dict(checkpoint[\"model\"])\n",
    "#checkpoint[\"model\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_path = \"/data/20bn-something-something-v2/v2-validation.json\"\n",
    "videos_folder = \"/data-ssd/v2-gulp-160/\"\n",
    "caption_type = \"label\"\n",
    "\n",
    "validation_parser = V2Parser(validation_path,\n",
    "                                            videos_folder,\n",
    "                               caption_type=caption_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def path_generator(annotation_path, root_path):\n",
    "    with open(annotation_path, \"rt\") as f:\n",
    "        annotations = json.load(f)\n",
    "    files = validation_parser.get_video_paths()\n",
    "    labels = validation_parser.get_captions()\n",
    "    return ((os.path.join(root_path, f), label) for f, label in zip(files, labels))\n",
    "path_gen = path_generator('/data/20bn-something-something-v2/v2-train.json', \n",
    "                          '/data/20bn-something-something-v2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rtorchn.data.fileio import MpegReader\n",
    "\n",
    "reader = MpegReader(12, (128, 128), keep_aspect_ratio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rtorchn.data.preprocessing import default_evaluation_preprocesser\n",
    "\n",
    "preprocessor = default_evaluation_preprocesser([48, 96, 96], 64.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ptcap.data.tokenizer import Tokenizer\n",
    "\n",
    "USER_MAXLEN=17\n",
    "tokenizer = Tokenizer(user_maxlen=USER_MAXLEN)\n",
    "tokenizer.load_dictionaries(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(net):\n",
    "    gradients = []\n",
    "\n",
    "\n",
    "    conv1_feats = net.module.encoder.c3d_extractor.conv1(video)\n",
    "    pool1_feats = net.module.encoder.c3d_extractor.pool1(conv1_feats)\n",
    "\n",
    "    conv2_feats = net.module.encoder.c3d_extractor.conv2(pool1_feats)\n",
    "    pool2_feats = net.module.encoder.c3d_extractor.pool2(conv2_feats)\n",
    "\n",
    "    conv3_feats = net.module.encoder.c3d_extractor.conv3(pool2_feats)  \n",
    "\n",
    "    pool3_feats = net.module.encoder.c3d_extractor.pool3(conv3_feats)  \n",
    "\n",
    "    conv4_feats = net.module.encoder.c3d_extractor.conv4(pool3_feats) \n",
    "    \n",
    "    \n",
    "    \n",
    "    conv5_feats = net.module.encoder.c3d_extractor.conv5(conv4_feats)   \n",
    "    conv6_feats = net.module.encoder.c3d_extractor.conv6(conv5_feats)   \n",
    "    ## REGISTER HOOK ##\n",
    "    conv6_feats.register_hook(lambda grad:gradients.append(grad))\n",
    "    \n",
    "    pool4_feats = net.module.encoder.c3d_extractor.pool4(conv6_feats)   \n",
    "\n",
    "    h = pool4_feats.view(pool4_feats.size()[0:3])   \n",
    "    h = h.permute(0, 2, 1)   \n",
    "    \n",
    "    h2 = net.module.encoder.c2d_extractor.extract_features(video)\n",
    "    h = torch.cat([h, h2], 2)\n",
    "\n",
    "    net.module.encoder.lstm.flatten_parameters()\n",
    "    h, _ = net.module.encoder.lstm(h) \n",
    "\n",
    "    net_output = net.module.decoder(h, Variable(torch.zeros([1, 1])).long().cuda())\n",
    "    return net_output, gradients, conv6_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAM calculating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cam(net, net_output, layer_feats, target_index, vocab_size, gradients, token_counter):\n",
    "\n",
    "    one_hot = np.zeros((1, vocab_size), dtype=np.float32)\n",
    "    one_hot[0][target_index] = 1\n",
    "    one_hot = Variable(torch.from_numpy(one_hot), requires_grad=True)\n",
    "    one_hot = torch.sum(one_hot.cuda() * net_output[:,token_counter,:])\n",
    "    net.module.encoder.c2d_extractor.conv4.zero_grad()\n",
    "    #net.module.classif_layer.zero_grad()\n",
    "    one_hot.backward(retain_graph=True)\n",
    "    \n",
    "    grads_val = gradients[0].cpu().data.numpy() #take the only element out of gradients list\n",
    "    feats_numpy = layer_feats.cpu().data.numpy()[0, :] #first sample of batch\n",
    "    weights = np.mean(grads_val, axis=(2, 3, 4))[0, :]   #grads_val: (1, 256, 48, 10, 10)\n",
    "    cam = np.ones(feats_numpy.shape[1:], dtype=np.float32)\n",
    "    weights = weights / weights.max()\n",
    "    for ii, ww in enumerate(weights):\n",
    "        cam += ww * feats_numpy[ii, :, :, :]\n",
    "#     cam = np.maximum(cam, 0)\n",
    "    return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path_to_video, label = next(path_gen)\n",
    "\n",
    "video_uint8 = reader.open(path_to_video)\n",
    "video = preprocessor(video_uint8)\n",
    "video = Variable(torch.from_numpy(video[None]), volatile=False).cuda()\n",
    "tokens = tokenizer.encode_caption(label)\n",
    "vocab_size = 2728\n",
    "\n",
    "print(label)\n",
    "all_cams = []\n",
    "for tt in range(0,1):\n",
    "\n",
    "    target_index =  tokens[tt]\n",
    "    \n",
    "    ##model\n",
    "    net_output, gradients, layer_feats = run_model(net)\n",
    "    cam = calc_cam(net, net_output, layer_feats, target_index, vocab_size, gradients, tt)\n",
    "    \n",
    "    print(tokenizer.decode_caption([target_index]))\n",
    "    print(\"*\"*15)\n",
    "    if target_index==1: #the token is <END>\n",
    "       break\n",
    "    for gg in range(0,video_uint8.shape[0]-7, 1):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        single_cam = cam[gg]\n",
    "        single_cam = cv2.resize(single_cam, (224, 224))\n",
    "        single_cam = single_cam - np.min(single_cam)\n",
    "        single_cam = single_cam / np.max(single_cam)\n",
    "        plt.imshow(single_cam)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(video_uint8[gg])\n",
    "        plt.show()\n",
    "        all_cams.append(single_cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cam.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path_to_video, label = next(path_gen)\n",
    "\n",
    "video_uint8 = reader.open(path_to_video)\n",
    "video = preprocessor(video_uint8)\n",
    "video = Variable(torch.from_numpy(video[None]), volatile=False).cuda()\n",
    "tokens = tokenizer.encode_caption(label)\n",
    "vocab_size = 2728\n",
    "\n",
    "print(label)\n",
    "all_cams = []\n",
    "for gg in range(0,video_uint8.shape[0]-7, 4):\n",
    "    cam = []\n",
    "    for tt in range(0,len(tokens)-1):\n",
    "        target_index =  tokens[tt]\n",
    "        ##model\n",
    "        net_output, gradients, layer_feats = run_model(net)\n",
    "        single_cam = calc_cam(net, net_output, layer_feats, target_index, vocab_size, gradients, tt)[gg]\n",
    "        single_cam = cv2.resize(single_cam, (224, 224))\n",
    "        single_cam = single_cam - np.min(single_cam)\n",
    "        single_cam = single_cam / np.max(single_cam)\n",
    "        cam.append(single_cam)\n",
    "#         print(tokenizer.decode_caption([target_index]))\n",
    "#         print(\"*\"*15)\n",
    "    single_cam = np.mean(np.array(cam), axis=0)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    \n",
    "    plt.imshow(single_cam)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(video_uint8[gg])\n",
    "    plt.show()\n",
    "    all_cams.append(single_cam)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    \"\"\"Unnormalize an tensor image with mean and standard deviation.\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor, i.e.\n",
    "    channel = (channel x std) + mean\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
    "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "            respecitvely.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = np.array(mean).astype('float32')\n",
    "        self.std = np.array(std).astype('float32')\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            self.mean = torch.FloatTensor(self.mean)\n",
    "            self.std = torch.FloatTensor(self.std)\n",
    "\n",
    "            if (self.std.dim() != tensor.dim() or\n",
    "                    self.mean.dim() != tensor.dim()):\n",
    "                for i in range(tensor.dim() - self.std.dim()):\n",
    "                    self.std = self.std.unsqueeze(-1)\n",
    "                    self.mean = self.mean.unsqueeze(-1)\n",
    "\n",
    "            tensor = torch.add(torch.mul(tensor, self.std), self.mean)\n",
    "        else:\n",
    "            # Relying on Numpy broadcasting abilities\n",
    "            tensor = tensor * self.std + self.mean\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unnormalize_op = UnNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "input_data_unnormalised = unnormalize_op(video.data.cpu().squeeze(0))\n",
    "input_data_unnormalised = input_data_unnormalised.permute(1, 2, 3, 0).numpy()  # (16x224x224x3)\n",
    "input_data_unnormalised = np.flip(input_data_unnormalised, 3)\n",
    "\n",
    "output_images_folder_cam_combined = os.path.join(\"cam_saved_images\", \"/home/farzaneh/cam/\", \"combined\")\n",
    "\n",
    "output_images_folder_original = os.path.join(\"cam_saved_images\", \"/home/farzaneh/cam/\", \"original\")\n",
    "output_images_folder_cam = os.path.join(\"cam_saved_images\", \"/home/farzaneh/cam\", \"cam\")\n",
    "\n",
    "os.makedirs(output_images_folder_cam_combined, exist_ok=True)\n",
    "os.makedirs(output_images_folder_cam, exist_ok=True)\n",
    "os.makedirs(output_images_folder_original, exist_ok=True)\n",
    "\n",
    "clip_size = len(all_cams)\n",
    "\n",
    "RESIZE_SIZE = 96\n",
    "RESIZE_FLAG = 1\n",
    "SAVE_INDIVIDUALS = 1\n",
    "for i in range(clip_size):\n",
    "    input_data_img = input_data_unnormalised[i, :, :, :]\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * all_cams[i]), cv2.COLORMAP_JET)\n",
    "    if RESIZE_FLAG:\n",
    "        input_data_img = cv2.resize(input_data_img, (RESIZE_SIZE, RESIZE_SIZE))\n",
    "        heatmap = cv2.resize(heatmap, (RESIZE_SIZE, RESIZE_SIZE))\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(input_data_img)\n",
    "    cam = cam / np.max(cam)\n",
    "    combined_img = np.concatenate((np.uint8(255 * input_data_img), np.uint8(255 * cam)), axis=1)\n",
    "    \n",
    "    cv2.imwrite(os.path.join(output_images_folder_cam_combined, \"img%02d.jpg\" % (i + 1)), combined_img)\n",
    "    if SAVE_INDIVIDUALS:\n",
    "        cv2.imwrite(os.path.join(output_images_folder_cam, \"img%02d.jpg\" % (i + 1)), np.uint8(255 * cam))\n",
    "        cv2.imwrite(os.path.join(output_images_folder_original, \"img%02d.jpg\" % (i + 1)), np.uint8(255 * input_data_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_combined_gif = os.path.join(output_images_folder_cam_combined, \"mygif.gif\")\n",
    "os.system(\"convert -delay 10 -loop 0 {}.jpg {}\".format(\n",
    "                                    os.path.join(output_images_folder_cam_combined, \"*\"),\n",
    "                                    path_to_combined_gif))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
