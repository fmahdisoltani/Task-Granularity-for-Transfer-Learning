{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Evaluation Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import HTML\n",
    "from skvideo.io import FFmpegReader, ffprobe, vwrite\n",
    "from torch.autograd import Variable\n",
    "from ptcap.trainers import DataParallelWrapper\n",
    "from ptcap.scores import ( caption_accuracy, first_token_accuracy, token_accuracy)\n",
    "from ptcap.data.annotation_parser import JsonParser\n",
    "from collections import OrderedDict\n",
    "from collections import Counter, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = ['A', 'AN', 'THE', '<END>']\n",
    "\n",
    "def safe_div(x,y):\n",
    "    if y == 0:\n",
    "        return 0\n",
    "    return x / y\n",
    "def fscore(precision, recall, beta=1):\n",
    "    numerator = (1.0 + (beta ** 2)) * precision * recall\n",
    "    denominator = ((beta ** 2) * precision) + recall\n",
    "    return {\"fscore\": safe_div(numerator, denominator)}\n",
    "class LCS(object):\n",
    "    \"\"\"\n",
    "    The main functionality of this class is to compute the LCS (Lowest Common\n",
    "    Subsequence) between a caption and prediction. By default, it returns the\n",
    "    precision and recall values calculated based on the LCS between a prediction\n",
    "    and a caption.\n",
    "    \"\"\"\n",
    "    def __init__(self, functions_list, tokenizer):\n",
    "        \"\"\"\n",
    "        Initializes functions_list and tokenizer.\n",
    "        Args:\n",
    "        functions_list: A list of the functions that will be applied on the\n",
    "        precision and recall values calculated based on the LCS between a\n",
    "        prediction and a caption.\n",
    "        \"\"\"\n",
    "\n",
    "        self.functions_list = functions_list\n",
    "        self.scores_container = OrderedDict()\n",
    "        self.scores_dict = OrderedDict()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, outputs):\n",
    "        string_predictions = [self.tokenizer.get_string(str_pred.data.numpy())\n",
    "                              for str_pred in outputs.predictions]\n",
    "        return self.score_batch(string_predictions, outputs.string_captions)\n",
    "\n",
    "    def collect_scores(self, batch_scores_dict, scores_dict):\n",
    "        for metric, metric_value in scores_dict.items():\n",
    "            if metric not in batch_scores_dict:\n",
    "                batch_scores_dict[metric] = [metric_value]\n",
    "            else:\n",
    "                batch_scores_dict[metric].append(metric_value)\n",
    "        return batch_scores_dict\n",
    "\n",
    "    @classmethod\n",
    "    def compute_lcs(cls, prediction, caption):\n",
    "        num_rows = len(prediction)\n",
    "        num_cols = len(caption)\n",
    "\n",
    "        table = [[0] * (num_cols + 1) for _ in range(num_rows + 1)]\n",
    "        for i in range(1, num_rows + 1):\n",
    "            for j in range(1, num_cols + 1):\n",
    "                if prediction[i - 1] == caption[j - 1]:\n",
    "                    table[i][j] = table[i - 1][j - 1] + 1\n",
    "                else:\n",
    "                    table[i][j] = max(table[i][j - 1], table[i - 1][j])\n",
    "        return table, table[num_rows][num_cols]\n",
    "\n",
    "    def mean_scores(self, batch_scores_dict):\n",
    "        for metric, metric_value in batch_scores_dict.items():\n",
    "            batch_scores_dict[metric] = np.mean(metric_value)\n",
    "        return batch_scores_dict\n",
    "\n",
    "    def score_batch(self, predictions, captions):\n",
    "        assert len(predictions) == len(captions)\n",
    "\n",
    "        batch_scores_dict = OrderedDict()\n",
    "        for count, (prediction, caption) in enumerate(zip(predictions,\n",
    "                                                          captions)):\n",
    "            scores_dict = self.score_sample(prediction.split(), caption.split())\n",
    "            batch_scores_dict = self.collect_scores(batch_scores_dict,\n",
    "                                                    scores_dict)\n",
    "\n",
    "        batch_scores_dict = self.mean_scores(batch_scores_dict)\n",
    "        return batch_scores_dict\n",
    "\n",
    "    def score_sample(self, prediction, caption):\n",
    "        scores_dict = OrderedDict()\n",
    "        _, lcs_score = self.compute_lcs(prediction, caption)\n",
    "        scores_dict[\"precision\"] = safe_div(lcs_score, len(prediction))\n",
    "        scores_dict[\"recall\"] = safe_div(lcs_score, len(caption))\n",
    "\n",
    "        for score_function in self.functions_list:\n",
    "            scores_dict.update(score_function(scores_dict[\"precision\"],\n",
    "                                              scores_dict[\"recall\"]))\n",
    "\n",
    "        return scores_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool to deal with mpeg videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_video(video_filenames):\n",
    "    \"\"\"\n",
    "    Tool to display videos inside the notebook.\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(video_filenames) is not list:\n",
    "        video_filenames = [video_filenames]\n",
    "    \n",
    "    html_code = ''\n",
    "    for filename in video_filenames:\n",
    "        video = io.open(filename, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        html_code += '''\n",
    "        <video alt=\"test\" width=\"640\" height=\"480\" controls>\n",
    "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "        </video>\n",
    "        '''.format(encoded.decode('ascii'))\n",
    "        \n",
    "    return HTML(data= html_code)\n",
    "\n",
    "\n",
    "def open_mpeg_video(fname, framerate, size):\n",
    "    \"\"\"\n",
    "    Open an mpeg video, and return it as a numpy array.\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = ffprobe(fname)\n",
    "    duration = float(metadata['video']['@duration'])\n",
    "    # Compute corresponding nb of frames\n",
    "    nframes = int(duration * framerate)\n",
    "    oargs = {\n",
    "        \"-r\": \"%d\" % framerate,\n",
    "        \"-vframes\": \"%d\" % nframes,\n",
    "        \"-s\": \"%dx%d\" % (size[0], size[1])\n",
    "    }\n",
    "    # Open file\n",
    "    reader = FFmpegReader(fname, inputdict={}, outputdict=oargs)\n",
    "    video = []\n",
    "    # Get frames until there is no more\n",
    "    for frame in reader.nextFrame():\n",
    "        video.append(frame)\n",
    "    # Return as a numpy array\n",
    "    return np.array(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CNN3dLSTMEncoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1f3d243b788f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mptcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptioners\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoderDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mptcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCNN3dLSTMEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mptcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained_encoders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFCEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJesterEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBIJesterEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mptcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTMDecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCoupledLSTMDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CNN3dLSTMEncoder'"
     ]
    }
   ],
   "source": [
    "from ptcap.model.captioners import EncoderDecoder\n",
    "from ptcap.model.encoders import CNN3dLSTMEncoder\n",
    "from ptcap.model.pretrained_encoders import FCEncoder, JesterEncoder, BIJesterEncoder\n",
    "from ptcap.model.decoders import LSTMDecoder, CoupledLSTMDecoder\n",
    "  \n",
    "#net = FullyConvolutionalNet(num_classes=178)jester1024_cutoff_300_ssssssss/\n",
    "\n",
    "net = EncoderDecoder(\n",
    "        encoder=,\n",
    "        decoder=CoupledLSTMDecoder,\n",
    "        encoder_kwargs={\"freeze\": False},#, \"pretrained_path\": \"/home/farzaneh/PycharmProjects/pretrained_nets/fully_conv_net_on_smtsmt_20170627/model.checkpoint\"},\n",
    "        decoder_kwargs={\"embedding_size\": 256, \"hidden_size\": 1024, \"num_lstm_layers\": 2, \n",
    "        \"vocab_size\": 2986, \"num_step\" :17}, \n",
    "        gpus=[0]).cuda()\n",
    "net = DataParallelWrapper(net, device_ids=[0]).cuda(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7f27f796f30c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "path = '/home/farzaneh/PycharmProjects/pytorch-captioning/results/cs5/Jan_two_stream_c2d_labels_cutoff5/'\n",
    "# path = '/home/farzaneh/PycharmProjects/pytorch-captioning/results/clapnet_captioning_only_f0.1'\n",
    "\n",
    "checkpoint = torch.load(path + '/model.best')\n",
    "\n",
    "\n",
    "net.load_state_dict(checkpoint[\"model\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ptcap.data.tokenizer import Tokenizer\n",
    "\n",
    "USER_MAXLEN=17\n",
    "tokenizer = Tokenizer(user_maxlen=USER_MAXLEN)\n",
    "tokenizer.load_dictionaries(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D) Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TMP_VIDEO_FILENAME = 'tmp.mp4'\n",
    "\n",
    "def unpreprocess(video):\n",
    "    video = video.data.numpy()[0]\n",
    "    video = 64. * video.transpose(1, 2, 3, 0)\n",
    "    return np.array(video, 'uint8')\n",
    "\n",
    "def demo(net, preprocessor, filename, top_n=5):\n",
    "    # Open mpeg file and get a numpy array\n",
    "    video_uint8 = open_mpeg_video(filename, 12, [128, 128])\n",
    "    # Preprocessing\n",
    "    video = preprocessor(video_uint8)\n",
    "    # Convert to torch variable\n",
    "    video = Variable(torch.from_numpy(video[None]), volatile=True).cuda()\n",
    "    empty_caption = Variable(torch.zeros([1, 1]), volatile=True).long().cuda()\n",
    "    \n",
    "    # Compute predictions\n",
    "    pred, class_pred = net.forward((video, empty_caption), use_teacher_forcing=False)\n",
    "    # Convert to numpy \n",
    "    pred = np.exp(pred.cpu().data.numpy())[0]\n",
    "        \n",
    "    pred_argmax = np.argmax(pred, axis=1)\n",
    "    decoded_pred = tokenizer.decode_caption(pred_argmax)\n",
    "    beautiful_caption = \" \".join(str(e+\" \") for e in decoded_pred if \"<END>\" not in e)\n",
    "    #print('__CAPTION__: {}'.format(beautiful_caption))\n",
    "    \n",
    "    \n",
    "    # Class index\n",
    "    class_index = torch.max(class_pred, dim=1)[1].cpu().data[0]\n",
    "    cls = int2label[class_index]\n",
    "    #print('ACTION: {:60s}\\n'.format(cls))\n",
    "    \n",
    "    \n",
    "    matched_action = get_template(decoded_pred, templates, tokenizer)\n",
    "    # print(actions)\n",
    "    objects = get_object_tokens(decoded_pred, matched_action[0][0])\n",
    "\n",
    "    \n",
    "    objects_list = extract_objects(objects)\n",
    "    # Print class name with proba\n",
    "    # Save input video in tmp file\n",
    "    vwrite(TMP_VIDEO_FILENAME, unpreprocess(video.cpu()))\n",
    "    return beautiful_caption, cls, objects_list, matched_action[0][0]\n",
    "\n",
    "\n",
    "def path_generator(annotation_path, root_path):\n",
    "    with gzip.open(annotation_path, \"rt\") as f:\n",
    "        annotations = json.load(f)\n",
    "    files = [elem['file'] for elem in annotations]\n",
    "    labels = [elem['label'] for elem in annotations]\n",
    "    placeholders = [elem['placeholders'] for elem in annotations] \n",
    "    actions = [elem['template'].replace(\"[\",\"\").replace(\"]\", \"\").replace(\",\",\"\").upper() for elem in annotations]\n",
    "    \n",
    "    return ((os.path.join(root_path, f), label, a, p) for f,label,p, a in zip(files, labels, placeholders, actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path generator\n",
    "path_gen = path_generator('/data/20bn-somethingsomething/json/test_20170929.json.gz', \n",
    "                          '/data/20bn-somethingsomething/videos')\n",
    "# Put the netwoark in evaluation mode\n",
    "_ = net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rtorchn.data.preprocessing import default_evaluation_preprocesser\n",
    "\n",
    "preprocessor = default_evaluation_preprocesser([48, 96, 96], 64.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(5000):\n",
    "    path_to_video, label, _,_ = next(path_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longest Common Subsequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_template(candidates, templates, tokenizer):\n",
    "   \n",
    "    lcs = LCS([fscore], tokenizer)\n",
    "    \n",
    "    max_templates = []\n",
    "    #print(\"There are {} templates\".format(len(templates)))\n",
    "\n",
    "    candidates = [\" \".join(candidates)]\n",
    "    for candidate in candidates:\n",
    "        \n",
    "        max_lcs_template = \"\"\n",
    "        max_lcs_value = -1\n",
    "        for template in templates:\n",
    "            lcs_value = compute_LCS(lcs, candidate, template, tokenizer)\n",
    "            if lcs_value > max_lcs_value:\n",
    "                max_lcs_template = template\n",
    "                max_lcs_value = lcs_value\n",
    "        max_templates.append((max_lcs_template, max_lcs_value))\n",
    "#         print(\"Candidate: {}\".format(candidate))\n",
    "        #print(\"MATCHED ACTION : {}\".format(max_lcs_template))\n",
    "\n",
    "    return max_templates\n",
    "\n",
    "\n",
    "def compute_LCS(lcs, candidate, template, tokenizer):\n",
    "    encoded_caption = Variable(\n",
    "        torch.LongTensor([tokenizer.encode_caption(candidate)]))\n",
    "    encoded_prediction = Variable(\n",
    "        torch.LongTensor([tokenizer.encode_caption(template)]))\n",
    "    score_attr = namedtuple(\"ScoresAttr\", \"string_captions captions predictions\")\n",
    "    in_tuple = score_attr([candidate], encoded_caption, encoded_prediction)\n",
    "    lcs_output = lcs(in_tuple)\n",
    "    return lcs_output['fscore']\n",
    "\n",
    "def extract_objects(object_tokens_list):\n",
    "    \n",
    "    objects_list = []\n",
    "    if len(object_tokens_list) == 0:\n",
    "        return objects_list\n",
    "    \n",
    "    next_token_ind =  object_tokens_list[0][0]\n",
    "    current_object = \"\"\n",
    "    for  (ind, token) in object_tokens_list:\n",
    "        if  next_token_ind == ind:\n",
    "            current_object += token+\" \"\n",
    "        else:\n",
    "            objects_list.append(current_object+\" \")\n",
    "            current_object = token\n",
    "            next_token_ind = ind\n",
    "        next_token_ind += 1\n",
    "        \n",
    "        \n",
    "    if len(current_object)>0:\n",
    "        objects_list.append(current_object)\n",
    "               \n",
    "    #print(\"PREDICTED OBJECTS: {}\".format(objects_list))\n",
    "    return objects_list\n",
    "\n",
    "\n",
    "\n",
    "def get_object_tokens(caption, template):\n",
    "    return [(i,token) for (i,token) in enumerate(caption) if token not in template and token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles= [\"the\", \"a\", \"an\", \"A\", \"An\", \"The\"]\n",
    "\n",
    "annotations = JsonParser.open_annotation(\"/data/20bn-somethingsomething/json/train_20171031.json.gz\")\n",
    "templates = np.unique(annotations[\"template\"]) # A list of templates\n",
    "objects = annotations[\"placeholders\"]\n",
    "obj_tokens = [token for token in objects if token not in stop_words]\n",
    "all_obj=[item for sublist in objects for item in sublist]\n",
    "filtered_obj =  [\" \".join(obj) for obj in all_obj for  token in obj if token not in articles]\n",
    "templates = [\" \".join(tokenizer.tokenize(t)) for t in templates]\n",
    "\n",
    "\n",
    "# get_objects(sentences1[0].split(), sentence1_templates[0].split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles= [\"the\", \"a\", \"an\", \"A\", \"An\", \"The\"]\n",
    "\n",
    "fil3 = [token.upper() for obj in all_obj for token in obj.split(\" \")]\n",
    "fil2 = [[token for token in obj.split(\" \") if token not in articles ] for obj in all_obj]\n",
    "fil = list( map(lambda p:\" \".join([token for token in p.split(\" \") if token not in articles ]), all_obj))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_objects = {}\n",
    "correct_objects = {}\n",
    "all_actions = {}\n",
    "correct_actions = {}\n",
    "\n",
    "for i in range(1000):\n",
    "    path_to_video, target_caption, target_action, target_objects = next(path_gen)\n",
    "    for p in target_objects:\n",
    "        p_tokens = p.split(\" \")\n",
    "        for pto in p_tokens:\n",
    "            all_objects[pto.strip().upper()] = all_objects.get(pto.strip().upper(), 0) + 1\n",
    "        \n",
    "    all_actions[target_action] = all_actions.get(target_action, 0) + 1\n",
    "\n",
    "   \n",
    "    print('sample {}'.format(i)) \n",
    "    pred_caption, pred_action, pred_objects, matched_action = demo(net, preprocessor, path_to_video)\n",
    "    for (i,o) in enumerate(pred_objects):\n",
    "        o_tokens = o.strip().split(\" \")\n",
    "        for oto in o_tokens:\n",
    "            if i<len(target_objects) and oto in target_objects[i].upper():\n",
    "                correct_objects[oto] = correct_objects.get(oto, 0) + 1\n",
    "                print(\"woohoo\")\n",
    "               \n",
    "          \n",
    "    print('TARGET CAPTION: {}'.format(target_caption))\n",
    "    print('PRED   CAPTION: {}\\n'.format(pred_caption))\n",
    "    print('TARGET  ACTION: {}'.format(target_action))\n",
    "    print('CLASSIF ACTION: {}'.format(pred_action))\n",
    "    print('CAPTION ACTION: {}\\n'.format(matched_action))\n",
    "            \n",
    "    print('TARGET OBJECTS: {}'.format(target_objects))\n",
    "    print('PRED   OBJECTS: {}\\n'.format(pred_objects))\n",
    "\n",
    "\n",
    "    if matched_action == target_action:\n",
    "        print(\"yesss\")\n",
    "        correct_actions[matched_action] = correct_actions.get(matched_action, 0) + 1\n",
    "        \n",
    "    print('{}\\n'.format('-'*65))\n",
    "    \n",
    "print(all_objects)\n",
    "print(correct_objects)\n",
    "\n",
    "print(all_actions)\n",
    "print(correct_actions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ca in correct_actions:\n",
    "    \n",
    "    print(\"{}/{} of {} actions correct\".format(correct_actions[ca], all_actions[ca], ca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for correct_key in correct_objects.keys():\n",
    "    denom = 0\n",
    "    if correct_key in all_objects.keys():\n",
    "        denom += all_objects[correct_key]\n",
    "        #print(\"{}:{}\".format(correct_key, all_objects[correct_key]))\n",
    "    #for j in all_objects.keys():\n",
    "    #    if correct_key in j or correct_key in j:\n",
    "    #        print(\"{}:{}\".format(j, all_objects[j]))\n",
    "    #        denom += all_objects[j]\n",
    "        \n",
    "                    \n",
    "    print (\">>model got  {}/{} of '{}'s correct\".format(correct_objects[correct_key],denom, correct_key ))\n",
    "    print(\"-\"*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = sum(correct_objects.values())\n",
    "b = sum(all_objects.values())\n",
    "\n",
    "print(\"{} out of {} objects are correctly predicted: {:.2}% \".format(a, b, a/b*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(all_objects.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = sum(correct_actions.values())\n",
    "d = sum(all_actions.values())\n",
    "\n",
    "print(\"{} out of {} actions are correctly predicted: {:}% \".format(c, d, c/d*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(all_actions.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
